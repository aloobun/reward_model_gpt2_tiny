{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ARJcaYlcw9i"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cg123/mergekit.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mergekit\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "qm824GHwc3yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create gpt2-small.yml file\n",
        "\n",
        "#slices:\n",
        "#  - sources:\n",
        "#    - model: gpt2\n",
        "#      layer_range: [0, 6]\n",
        "#merge_method: passthrough\n",
        "#dtype: float16\n"
      ],
      "metadata": {
        "id": "QnBNLhWwc30k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mergekit-yaml /gpt2-small.yml ./gpt2-small"
      ],
      "metadata": {
        "id": "QzdaBkVec33F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "import json\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "wo9sSjhVc35b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"/gpt2-small\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"/gpt2-small\", num_labels=2)\n",
        "model.to(device)\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ],
      "metadata": {
        "id": "_gmXhFZbc376"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_input_ids = []\n",
        "rejected_input_ids = []\n",
        "chosen_attention_mask = []\n",
        "rejected_attention_mask = []\n",
        "\n",
        "dataset = load_dataset(\"winglian/no_robots_rlhf\")\n",
        "train_dataset = dataset['train']\n",
        "prompts = [item['prompt'] for item in train_dataset]\n",
        "chosen = [item['chosen'] for item in train_dataset]\n",
        "rejected = [item['rejected'] for item in train_dataset]\n",
        "max_length = 512\n",
        "encodings = tokenizer(prompts, chosen, rejected, truncation=True, padding='max_length', max_length=max_length)\n",
        "for i, prompt  in enumerate(prompts):\n",
        "    chosen_input_ids.append(tokenizer(prompts[i], chosen[i], truncation=True, padding='max_length', max_length=max_length)['input_ids'])\n",
        "    chosen_attention_mask.append(tokenizer(prompts[i], chosen[i], truncation=True, padding='max_length', max_length=max_length)['attention_mask'])\n",
        "    #answer2 is the rejected answer\n",
        "    rejected_input_ids.append(tokenizer(prompts[i], rejected[i], truncation=True, padding='max_length', max_length=max_length)['input_ids'])\n",
        "    rejected_attention_mask.append(tokenizer(prompts[i], rejected[i], truncation=True, padding='max_length', max_length=max_length)['attention_mask'])\n",
        "\n",
        "chosen_input_ids = torch.tensor(chosen_input_ids).to(device)\n",
        "rejected_input_ids = torch.tensor(rejected_input_ids).to(device)\n",
        "chosen_attention_mask = torch.tensor(chosen_attention_mask).to(device)\n",
        "rejected_attention_mask = torch.tensor(rejected_attention_mask).to(device)\n",
        "\n",
        "dataset = TensorDataset(chosen_input_ids, chosen_attention_mask, rejected_input_ids, rejected_attention_mask)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "9Mbcy3vjc3-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    for i, batch in enumerate(loader):\n",
        "\n",
        "        chosen_input_ids, chosen_attention_mask, rejected_input_ids, rejected_attention_mask = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rewards_chosen = model(input_ids=chosen_input_ids, attention_mask=chosen_attention_mask)[0]\n",
        "\n",
        "        rewards_rejected = model(input_ids=rejected_input_ids, attention_mask=rejected_attention_mask)[0]\n",
        "\n",
        "        loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Batch {i+1}/{len(loader)}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "RdTOnyHbc4Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"reward_model\"\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "yIeaHInnc4DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w6Mgw2r2c4Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPQLUUVoc4H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zNwfMxiVc4KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l8eb_4Auc4Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ws-KpXItc4PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "onIbItNyc4Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lLo7TZ33c4UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bDrtNc3mc4Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "peuXQcOnc4ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pa2DHC65c4bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lpj0d6Jqc4d_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}